# Statistical inference
## Week 1 
### Video 2
__Prob density funcs__
plot(x,y, lwd = 3, frame = FALSE, type = "l")

__Beta Distribution__
pbeta(0.75,2,1) 
p <- in front of beta asks for probility

pbeta(0.75,2,1)

Cum Dist Func CDF 
F(x) = P(X <= x)
and Suvuval Functino
F(x) = P(X > x)
note: S(x) = 1-F(x) = 1 - x^2

__Quantiles__
F(x of alpha) = alpha
where alpha = 95% of total pop

qbeta(0.5,2,1)
q in front of func density name gives you a quantile


### Video 3
Conditional Probability
^ = intersect
P(A|B) = P(A^B) / P(B)

independant
P(A|B) = P(A)P(B) / P(B)

## Week 2
### Video 1 - Variability

```{r}
nosim  <- 1000
n <- 10
# -- This code below creates a 1000 x 1000 matrix with normally 
# -- distributed random variables. Then it takes the mean of  
# -- each row of 1000 rand numbers and makes a tall vector.
# -- Then it takes the standard deviation of the tall vector. 
sd(apply(matrix(rnorm(nosim * n ), nosim), 1, mean))
# These should be the same
1/sqrt(n)
```
```{r}
Poisson(4) have variance 4; means of random samples of n Poisson(4) have sd 2/sqrt(n)

nosim <- 1000
n <- 10
sd(apply(matrix(rpois(nosim * n, 4), nosim), 1, mean))

Var(X) = sigma^2 / n

E[X] = sqrt(sigma^2) / sqrt(n)
Var(X) = 4 = sigma^2 so...
sqrt(4) / sqrt(n)

```
```{r}
# Simulate Coin Flips
Var(X) = 0.25
E[X] = mean = sigma / (2*sqrt(n))

nosim <- 1000
n <- 10
sd(apply(matrix(sample(0:1, nosim*n, replace = TRUE), nosim), 1, mean))

# Get Mean
1/(2*sqrt(n))



```

```{r}
library(UsingR)
data(father.son)
x <- father.son$sheight
n <- length(x)

# Take a look at some information we know

round(c(var(x), var(x)/n, sd(x), sd(x)/sqrt(n)),2)


```

#### Key take aways from Video 2
- Sample Variance estimates the population variance
- The Distribution of the sample variance is centered at what it is estimating
- It get more conentrated around the population variance with large samples
- Var(Xsample) = Var(Xpop) / n
- Standard Error = Sqrt(Var(Xpop) / n)


### Video 2
#### E.g. 1 Website font changed to comic sans, brings in more revenue

P(9 or 10 sites have more revenue) = 
choose(10, 9)*0.5^9*(1-0.5)+choose(10, 10)*0.5^10*1 

or

pbinom(10/4*3, size=10, prob=0.5, lower.tail=FALSE     )

__Q__
Software company documentation errors
pnorm(-3,mean=11,sd=2, lower.tail=TRUE)


#### Poisson Distbn
__Q__
ppois(40, lambda=9*5)


### Video 3 - Asymptotics
__Q__
P(H<=45) from 100 coin flips
E[Xi] = 0.5 , Var(Xi) = 0.5(1-0.5)

Std Err = sqrt(0.5*(1-0.5)/100)
= 0.05

(0.508-0.5)*2*sqrt(100)

#### Confidence Intervals
library(UsingR)
data(father.son)
x <- father.son$sheight
(mean(x) + c(-1,1)*qnorm(0.975) * sd(x)/sqrt(length(x)))/12
__Q__
mean(x) + c(-1,1) * 2 * sd(x)/sqrt(length(x))

__Useful Confidence Interval__
binom.test(56,100)$conf.int

__Nuclear Pump Failure__
```{r}
Poisson Distbn
x <- 5
t <- 94.32
lambda <- x/t

# Approx Poisson interval
round(lambda + c(-1,1) * qnorm(0.975) * sqrt(lambda/t), 3)
>> 0.007 0.099

# Exact Poisson interval
poisson.test(x, T=94.32)$conf

```


__Example__ 
Search entries into website = 10 per min
monitoring 1hr.
Find Exact poisson interval for rate of events per minute?

```{r}
x <- 10
t <- 60
lambda <- x/t
# Approx Poisson interval
round(lambda + c(-1,1) * qnorm(0.975) * sqrt(lambda/t), 3)

# Exact Poisson interval
poisson.test(x, T=60)$conf

```

mu = 15
sigma = 10 
n = 100 







#### Quiz 2
__Q2__
pnorm(70, mean=80, sd=10, lower.tail=TRUE)

__Q3__
1100+75*0.95

qnorm(0.95, mean=1100, sd=75)

__Q4__
pnorm(1115,mean=998.36, sd=75, lower.tail=TRUE)

__Q5__
choose(5,4)*0.5^4*(1-0.5) + choose(5,5)*0.5^5*1

__Q6__
pnorm(16, mean=15, sd=10) - pnorm(14, mean=15, sd=10)

__Q8__
sqrt(1/12)

__Q9__
ppois(10, lambda = 5*3)

__Q10__
pbinom(2, size=1000, prob=0.5)

## Week 3
### Video 1 - t-Confidence Intervals

```{r}
#
# Code to plot Normal Z distbn vs T-distbn (with funky manipulate slider)
#
require(manipulate)
require(ggplot2)
k <- 1000
xvals <- seq(-5, 5, length=k)
myplot <- function(df){
      d <- data.frame(y=c(dnorm(xvals), dt(xvals, df)), 
                        x=xvals, 
                        dist = factor(rep(c("Normal","T"), c(k,k))))
      g <- ggplot(d, aes(x=x, y=y))
      g <- g+ geom_line(size=2, aes(colour =dist))
      g
}
manipulate(myplot(mu), mu=slider(1,20, step=1))


require(manipulate)
require(ggplot2)
pvals <- seq(.5, .99, by=.01)
myplot2 <- function(df){
      d <- data.frame(n=qnorm(pvals), t=qt(pvals, df), p=pvals)
      
      g <- ggplot(d, aes(x=n, y=t))
      g <- g+geom_abline(size = 2, col = "lightblue") 
      g <- g+ geom_line(size=2, col="black")
      g <- g+ geom_vline(xintercept = qnorm(0,975))
      g <- g+ geom_hline(yintercept = qt(0.975, df))
      g
}
manipulate(myplot2(df), df=slider(1,20, step=1))

```

```{r}
# Gosset's sleep data
data(sleep)
head(sleep)

g1 <- sleep$extra[1:10]; g2 <- sleep$extra[11:20]
difference  <- g2-g1
mn <- mean(difference); s <- sd(difference); n <- 10

# Confidence Interval df=n-1
## All these methods give you the same results for confidence intervals
mn+c(-1,1)*qt(.975, n-1) * s / sqrt(n)  # 1
t.test(difference) # 2
t.test(g2, g1, paired = TRUE) # 2
t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep) # 4


```

/*
---
title: ''your title here"
author: "your name here"
date: "your date here"
output: 
  pdf_document:
    highlight: default
    fig_width: 4
    fig_height: 3
fontsize: 11pt
geometry: margin=0.20in
---
*/

#### Confidence Intervals
__Contraceptive example__
```{r}
n_OC = 8
n_C = 21
X_bar_OC = 132.86, S_OC = 15.34
X_bar_C = 127.44, S_C = 18.23

# Calculate using the pooled variance estimator:
sp  <- sqrt((7 * 15.34^2 + 20 * 18.23^2) / (8 + 21 -2))
# Now find the confidence interval
132.86 - 127.44 + c(-1,1) * qt(0.975, 27) * sp * (1/8 + 1/21)^0.5

```
__ Mistakenly treating the sleep data as grouped__
```{r}
n1 <- length(g1); n2 <- length(g2)
sp <- sqrt(((n1-1)*sd(x1)^2 + (n2-1) * sd(x2)^2) / (n1 + n2-2))
md <- mean(g2) - mean(g1)
semd <- sp * sqrt( 1 / n1 + 1/n2)
rbind(
      md + c(-1,1) * qt(.975, n1 + n2 - 2) * semd,
      t.test(g2, g1, paired = FALSE, var.equal = TRUE)$conf,
      t.test(g2, g1, paired = TRUE)$conf
)

```

```{r}
library(datasets); data(ChickWeight); library(reshape2)
## define weight gain or loss
wideCW  <- dcast(ChickWeight, Diet + Chick ~ Time, value.var = "weight")
names(wideCW)[-(1:2)]  <- paste("time", names(wideCW)[-(1:2)], sep ="")
require("dplyr")
wideCW  <- mutate(wideCW, 
      gain = time21 - time0                  
)

## Consider using a violin plot?? to plot gains

# Let's do a t interval
# assumption: not sure if vairances are equal or unequal, so lets do both (paired option below)
wideCW14  <- subset(wideCW, Diet %in% c(1,4))
rbind(
      t.test(gain ~ Diet, paired = FALSE, var.equal = TRUE, data = wideCW14)$conf,
      t.test(gain ~ Diet, paired = FALSE, var.equal = FALSE, data = wideCW14)$conf
)

```

__Example__
Comparing SBP for 8 oral contraceptives and 21 controls
```{r}
n_OC = 8
n_C = 21
X_bar_OC = 132.86, S_OC = 15.34
X_bar_C = 127.44, S_C = 18.23

df = 15.04  ## Calculated by a huge degrees of freedom calculation
t_1504_0975 = 2.13

# Now calculate the ~95% Confidence interval for this experiment

132.86 - 127.44 + c(-1,1) * 2.13 * sqrt(15.34^2 / 8 + 18.23^2 / 21)

# Using R functions

t.test(...., var.equal= FALSE)


```

### Video 2 - Week 3
Hypothesis testing

The probability if rejecting the null hypothesis when it is false is called the power
Power is used to calculate the sample size for experiments

__T- Test in R__
library(UsingR); 
data(father.son) t.test(father.son$sheight - father.son$fheight)


__Chicken Weigth - Unequal Variance T-Test on Diets 1 & 4__
library(datasets); data(ChickWeight); library(reshape2)

##define weight gain or loss
wideCW <- dcast(ChickWeight, Diet + Chick ~ Time, value.var = "weight") 
names(wideCW)[-(1 : 2)] <- paste("time", names(wideCW)[-(1 : 2)], sep = "") 

library(dplyr)
wideCW <- mutate(wideCW, gain = time21 - time0 )

wideCW14 <- subset(wideCW, Diet %in% c(1, 4)) 
t.test(gain ~ Diet, paired = FALSE, var.equal = FALSE, data = wideCW14)



### Quiz Week 3
```{r}

## Q1
n = 9
X_bar = 1100
s = 30
mu = 
degfree = n-1      
#3 What is 95% CI for the mean brain volume?

# X_bar +/- tn-1 * s / sqrt(n)
round(X_bar + c(-1,1) * qt(0.975, degfree) * s / sqrt(n))

## Q2
n = 9
t = 6 wks
X_bar = -2 
# s = ?
# CI = 95%
degfree = n-1
upperLimit = 0
# lowerLimit = ?

# upperlimit = X_bar +/- tn-1 * s / sqrt(n)
# rearranging

s = ( (upperLimit - X_bar) * sqrt(n) ) / qt(0.975, degfree)
s

## Q3 
# 
# Paired interval since in the chicken example there were 2 diets 
# and 2 separate groups of chickens. This was not a paired study. 
# With the runners, the same runners were tested twice each on different 
# supplements so the runner is common amongst both tests. 


## Q4
n = 20 
10 nights new and 10 nights old
Nightly median waiting time = MWT

# Y
MWT_new = 3
var_new = 0.6
n_new = 10

# X
MWT_old = 5
var_old = 0.68
n_old = 10
alpha = 0.05

degfree = n_new + n_old - 2

Sp = sqrt( 
            ( (n_old - 1) * var_old + (n_new - 1) * var_new ) /  degfree     
)

# t for 95%
round(MWT_new - MWT_old + c(-1,1) * qt(1 - alpha/2, degfree) * Sp * sqrt(1/n_old + 1/n_new), 2)


## Q5
Interval will be narrower

## Q6
# Y
MWT_new = 4
s_new = 0.5
n_new = 100
stdErr_new = s_new / sqrt(n_new)

# X
MWT_old = 6
s_old = 2
n_old = 100
stdErr_old = s_old / sqrt(n_old)

(s_old - s_new) / sqrt(200)

alpha = 0.05

# H0 = descrease in MWT_new
# 95% 

var.equal = FALSE

Zquartile = qnorm(0.95)

# Using CI = X_bar + stdErr * Zquartile 
CI = MWT_old-MWT_new + c(-1,1) * (stdErr_old - stdErr_new) * Zquartile

# If the interval contains zero then sometime the old system wait time 
# would be less than the new system's wait time. This would not be evidence 
# that the new system is effective

## Q7
n_pill = 9
x_pill = -3
s_pill = 1.5
stdErr_pill = s_pill / sqrt(n_pill)

n_placebo = 9
x_placebo = 1 
s_placebo = 1.8 
stdErr_placebo = s_placebo / sqrt(n_placebo)

stdErr = (s_pill + s_placebo) / sqrt(n_pill + n_placebo)

# H0 = decrease in treated (pill) group??
# 90% 

Zquartile = qnorm(0.975)


# Using CI = X_bar + stdErr * Zquartile 
# Pill - Placebo
CI = x_pill - x_placebo + c(-1,1) * (stdErr_pill - stdErr_placebo) * Zquartile
(x_pill - x_placebo) + c(-1,1) * stdErr * Zquartile
CI


```





# Project notes
round(t.test(len~supp, paired=FALSE, var.equal=FALSE, conf.level=0.95, data=t05)$conf, 3)
you can replace “$conf” with “$statistic” or “$p.value”



## Week 4 Videos
?power.t.test
__Uses__
- Can be used to calculate the power given n, delta, sd, type, and alt
power.t.test(n = 16, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$power

- can be used to calculate the necessary sample size given power, delta, sd, type, and alt
power.t.test(power = 0.8, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$n

- can be used to calculate the minimum detectable delta
power.t.test(power = 0.8, n=100, sd = 1, type = "one.sample", alt = "one.sided")$delta

__Boot Strapping and Jack Knifing__
The Jack Knife Routine only requires the vector 'x' and the function to be jack knifed e.g. median or mean.

?jackknife
library(bootstrap)
temp <- jackknife(x, median) 
c(temp$jack.bias, temp$jack.se)

__Permutation test for pesticide data__
data(InsectSprays)
boxplot(count ~ spray, data = InsectSprays)

subdata <- InsectSprays[InsectSprays$spray %in% c("B", "C"), ]
y <- subdata$count
group <- as.character(subdata$spray)
testStat <- function(w, g) mean(w[g == "B"]) - mean(w[g == "C"]) 
observedStat <- testStat(y, group)
permutations <- sapply(1:10000, function(i) testStat(y, sample(group))) 
observedStat

mean(permutations > observedStat)

### Video 2 - Multiple Comparisons
p.adjust(pValues, method="bonferroni") 



## Quiz 4 - Week 4

Q1
```{r}
H0 = there was a mean reduction in blood pressure = mu0 > mu
Ha = there was no mean reduction in blood pressure = mua <= mu

baseline  <-  c(140, 138, 150, 148, 135)
week2  <-  c(132, 135, 151, 146, 130)
pressureDelta  <-  baseline - week2

mean_delta = mean(delta)

t.test(pressureDelta, paired=FALSE, var.equal=FALSE, conf.level=0.95)$p.value
round(
      t.test(baseline,week2 , paired=TRUE, var.equal=FALSE, conf.level=0.95)$p.value
      ,3)
# 0.087
```

Q2
```{r}
# mu0 = ?
# for H0: mu = mu0
# Fail to reject Null Hyp, therefore -> mu = mu0, 95% conf


X_bar = 1100
mu = 1100
s = 30
n = 9
TS = (X_bar - mu0) / (s / sqrt(n))

#let 
alpha = 0.0125
Z_alpha = qnorm(alpha) 
Z_alpha
# Reject Null Hypothesis when
if ( TS <= Z_alpha ) 'Reject H0' else 'Fail to Reject H0'


mu0  <- X_bar - c(-1,1)*( (Z_alpha*s) / sqrt(n) )
round(mu0,0)
```

Q3
```{r}
n = 4 
coke = 3
pepsi = 1
p_coke = 0.5

choose(n,coke)*p_coke^2 + choose(n,n)*p_coke^2

round(pbinom(2,size=4,prob=0.5, lower.tail=FALSE), 2)


```


Q4
```{r}
# H0: lambda0 = 0.01
lambda0 = 1/100 = lambda0_1787 = 17.87 per 1787 days
lambdaA = 10/1787
n = 10
mm = 1787/100

?ppois
round( ppois(9, 17.87 , lower.tail = TRUE), 3)

```

Q5
```{r}
n_pill = 9
n_plac = 9

x_pill = -3
x_plac = 1

s_pill = 1.5
s_plac = 1.8

?rnorm
pill = rnorm(n=9, mean=x_pill, sd=s_pill)
plac = rnorm(n=9, mean=x_plac, sd=s_plac)

pValue  <- t.test(pill, plac, paired=FALSE, var.equal=FALSE, conf.level=0.95)$p.value
if(pValue<0.01) 'Less than 0.01' 

```


Q6
```{r}
n = 9
CI = 1077 , 1123

mu = 1078

Z = qnorm(0.9)

s = 46 / (Z*2)
X_bar = 1123 - Z*s


mu = 1100

TS = (X_bar - mu0) / (s / sqrt(n))

#let 
alpha = 0.025  # (two sided 5% hypothesis test) 
Z_alpha = qnorm(alpha) 
Z_alpha
# Reject Null Hypothesis when
if ( TS <= Z_alpha ) 'Reject H0' else 'Fail to Reject H0'


mu0  <- X_bar - c(-1,1)*( (Z_alpha*s) / sqrt(n) )
round(mu0,0)
```

Q7
```{r}
n=100
x_bar = 0.01
s = 0.04
# power=?
# H0: mu0 = mu no volume loss
round(
      power.t.test(n = 100, delta = 0.01, sd = 0.04, type = "one.sample", alt = "one.sided")$power
,2)


```

Q8 
```{r}
using values from above

round(
      power.t.test(power = 0.9, delta = 0.01, sd = 0.04, type = "one.sample", alt = "one.sided")$n
,0)

```


Q10
```{r}

n_metro = 288
x_bar_metro = 44
s_metro = 12


n_gotham = 288
x_bar_gotham = 42.04
s_gotham = 12

metro = rnorm(n=n_metro, mean=x_bar_metro, sd=s_metro)
gotham = rnorm(n=n_gotham, mean=x_bar_gotham, sd=s_gotham)

#?t.test
pValue  <- t.test(metro, gotham, paired=FALSE, var.equal=FALSE, conf.level=0.95)$p.value
round(pValue,2)

```



Q11
```{r}
m = 10
FWER <= 0.05
alpha = 0.05
alpha_fwer = alpha / m 
alpha_fwer
# All pvalues < alpha_fwer are significant

p.adjust(pValues, method="bonferroni") 





```

